#!/usr/bin/env python3
"""
ç‰¹æœ—æ™®å¸–å­å­˜æ¡£å™¨
åŠŸèƒ½ï¼š
1. è·å–å¹¶å­˜æ¡£æ‰€æœ‰å†å²å¸–å­
2. å®æ—¶ç›‘æ§æ–°å¸–å­
3. æä¾›å¸–å­æŸ¥è¯¢å’Œéå†åŠŸèƒ½
"""

import feedparser
import time
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
import re
import os
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('post_archiver.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class TrumpPostArchiver:
    """ç‰¹æœ—æ™®å¸–å­å­˜æ¡£å™¨"""
    
    def __init__(self, rss_url: str = "https://trumpstruth.org/feed", archive_file: str = None):
        self.rss_url = rss_url
        # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿æ–‡ä»¶ä½ç½®å›ºå®š
        base_dir = Path(__file__).parent
        self.archive_file = archive_file or str(base_dir / 'trump_posts_archive.json')
        self.posts_dict = {}  # ä½¿ç”¨å­—å…¸å­˜å‚¨ï¼Œkeyä¸ºpost_id
        self.load_archive()
    
    def load_archive(self):
        """åŠ è½½å·²å­˜æ¡£çš„å¸–å­"""
        try:
            if os.path.exists(self.archive_file):
                with open(self.archive_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.posts_dict = data.get('posts', {})
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å­˜æ¡£å¤±è´¥: {e}")
            self.posts_dict = {}
    
    def save_archive(self):
        """ä¿å­˜å¸–å­å­˜æ¡£"""
        try:
            # æŒ‰æ—¶é—´æ’åºå¸–å­
            sorted_posts = dict(sorted(
                self.posts_dict.items(),
                key=lambda x: x[1].get('timestamp', ''),
                reverse=True
            ))
            
            data = {
                'total_posts': len(sorted_posts),
                'last_updated': datetime.now().isoformat(),
                'posts': sorted_posts
            }
            
            with open(self.archive_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å­˜æ¡£å¤±è´¥: {e}")
    
    def fetch_rss_feed(self) -> Optional[feedparser.FeedParserDict]:
        """è·å–RSS Feed"""
        try:
            feed = feedparser.parse(self.rss_url)
            
            if feed.bozo:
                logger.warning(f"âš ï¸ RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
            
            return feed
            
        except Exception as e:
            logger.error(f"âŒ è·å–RSS Feedå¤±è´¥: {e}")
            return None
    
    def extract_post_data(self, entry) -> Optional[Dict]:
        """ä»RSSæ¡ç›®æå–å¸–å­æ•°æ®"""
        try:
            # æå–å¸–å­ID
            post_id = None
            if hasattr(entry, 'link'):
                match = re.search(r'/statuses/(\d+)', entry.link)
                if match:
                    post_id = match.group(1)
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å†…å®¹å“ˆå¸Œ
                    post_id = f"hash_{abs(hash(entry.title + entry.description))}"
            
            if not post_id:
                return None
            
            # æå–å¸–å­æ–‡æœ¬
            post_text = ""
            if hasattr(entry, 'description'):
                # æ¸…ç†CDATAæ ‡ç­¾
                post_text = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', entry.description)
                # ç§»é™¤HTMLæ ‡ç­¾
                post_text = re.sub(r'<[^>]+>', '', post_text)
                post_text = post_text.strip()
            
            # å¦‚æœæ˜¯ç©ºæ ‡é¢˜æˆ–æ— å†…å®¹ï¼Œå°è¯•ä½¿ç”¨title
            if not post_text or len(post_text) < 10:
                if hasattr(entry, 'title'):
                    post_text = entry.title
            
            # æå–æ—¶é—´æˆ³
            timestamp = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                timestamp = datetime(*entry.published_parsed[:6]).isoformat()
            elif hasattr(entry, 'published'):
                timestamp = entry.published
            
            # æå–URL
            post_url = entry.link if hasattr(entry, 'link') else None
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºè½¬å‘
            is_retweet = post_text.startswith('RT @realDonaldTrump')
            
            if post_text:
                return {
                    'id': post_id,
                    'text': post_text,
                    'timestamp': timestamp,
                    'url': post_url,
                    'is_retweet': is_retweet,
                    'scraped_at': datetime.now().isoformat(),
                    'character_count': len(post_text)
                }
        
        except Exception as e:
            logger.error(f"âŒ æå–å¸–å­æ•°æ®å¤±è´¥: {e}")
        
        return None
    
    def fetch_and_archive_all(self) -> int:
        """è·å–å¹¶å­˜æ¡£æ‰€æœ‰å½“å‰å¯è§çš„å¸–å­"""
        feed = self.fetch_rss_feed()
        if not feed:
            return 0
        
        new_count = 0
        updated_count = 0
        
        for entry in feed.entries:
            post_data = self.extract_post_data(entry)
            if post_data:
                post_id = post_data['id']
                
                if post_id not in self.posts_dict:
                    # æ–°å¸–å­
                    self.posts_dict[post_id] = post_data
                    new_count += 1
                else:
                    # æ›´æ–°ç°æœ‰å¸–å­
                    self.posts_dict[post_id].update(post_data)
                    updated_count += 1
        
        self.save_archive()
        return new_count
    
    def monitor_new_posts(self, interval: int = 30, callback=None):
        """å®æ—¶ç›‘æ§æ–°å¸–å­"""
        try:
            while True:
                feed = self.fetch_rss_feed()
                if feed:
                    new_posts = []
                    
                    for entry in feed.entries:
                        post_data = self.extract_post_data(entry)
                        if post_data:
                            post_id = post_data['id']
                            
                            if post_id not in self.posts_dict:
                                # å‘ç°æ–°å¸–å­
                                self.posts_dict[post_id] = post_data
                                new_posts.append(post_data)
                    
                    if new_posts:
                        self.save_archive()
                        
                        # å¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼Œè°ƒç”¨å®ƒ
                        if callback:
                            for post in new_posts:
                                callback(post)
                        
                        # æ˜¾ç¤ºæ–°å¸–å­
                        for post in new_posts:
                            self.display_post(post)
                
                time.sleep(interval)
                
        except KeyboardInterrupt:
            self.save_archive()
        except Exception as e:
            logger.error(f"âŒ ç›‘æ§å‡ºé”™: {e}")
            self.save_archive()
    
    def display_post(self, post: Dict):
        """æ˜¾ç¤ºå¸–å­è¯¦æƒ…"""
        print("\n" + "=" * 80)
        print("ğŸ†• æ–°å¸–å­")
        print("=" * 80)
        print(f"ğŸ†” ID: {post['id']}")
        print(f"ğŸ“… æ—¶é—´: {post.get('timestamp', 'æœªçŸ¥')}")
        print(f"ğŸ”— é“¾æ¥: {post['url']}")
        print(f"ğŸ”„ è½¬å‘: {'æ˜¯' if post.get('is_retweet') else 'å¦'}")
        print(f"ğŸ“ å­—æ•°: {post['character_count']}")
        print("=" * 80)
        print("ğŸ“ å†…å®¹:")
        print("-" * 80)
        print(post['text'])
        print("=" * 80)
        print(f"ğŸ“Š å­˜æ¡£æ€»æ•°: {len(self.posts_dict)}")
        print("=" * 80 + "\n")
    
    def get_all_posts(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¸–å­ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰"""
        posts = list(self.posts_dict.values())
        posts.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        return posts
    
    def get_post_by_id(self, post_id: str) -> Optional[Dict]:
        """æ ¹æ®IDè·å–å¸–å­"""
        return self.posts_dict.get(post_id)
    
    def search_posts(self, keyword: str) -> List[Dict]:
        """æœç´¢åŒ…å«å…³é”®è¯çš„å¸–å­"""
        results = []
        keyword_lower = keyword.lower()
        
        for post in self.posts_dict.values():
            if keyword_lower in post['text'].lower():
                results.append(post)
        
        results.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        return results
    
    def get_posts_by_date(self, date_str: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„å¸–å­ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰"""
        results = []
        
        for post in self.posts_dict.values():
            if post.get('timestamp', '').startswith(date_str):
                results.append(post)
        
        results.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        return results
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        posts = list(self.posts_dict.values())
        
        if not posts:
            return {
                'total_posts': 0,
                'retweets': 0,
                'original_posts': 0
            }
        
        retweets = sum(1 for p in posts if p.get('is_retweet'))
        total_chars = sum(p.get('character_count', 0) for p in posts)
        
        # è·å–æœ€æ—©å’Œæœ€æ–°çš„æ—¶é—´æˆ³
        timestamps = [p.get('timestamp') for p in posts if p.get('timestamp')]
        timestamps.sort()
        
        return {
            'total_posts': len(posts),
            'retweets': retweets,
            'original_posts': len(posts) - retweets,
            'average_length': total_chars / len(posts) if posts else 0,
            'earliest_post': timestamps[0] if timestamps else None,
            'latest_post': timestamps[-1] if timestamps else None
        }
    
    def export_to_text(self, output_file: str = 'trump_posts_export.txt'):
        """å¯¼å‡ºæ‰€æœ‰å¸–å­åˆ°æ–‡æœ¬æ–‡ä»¶"""
        try:
            posts = self.get_all_posts()
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("ç‰¹æœ—æ™®Truth Socialå¸–å­å­˜æ¡£\n")
                f.write("=" * 80 + "\n")
                f.write(f"å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»å¸–å­æ•°: {len(posts)}\n")
                f.write("=" * 80 + "\n\n")
                
                for i, post in enumerate(posts, 1):
                    f.write(f"\n{'=' * 80}\n")
                    f.write(f"å¸–å­ #{i}\n")
                    f.write(f"{'=' * 80}\n")
                    f.write(f"ID: {post['id']}\n")
                    f.write(f"æ—¶é—´: {post.get('timestamp', 'æœªçŸ¥')}\n")
                    f.write(f"é“¾æ¥: {post['url']}\n")
                    f.write(f"ç±»å‹: {'è½¬å‘' if post.get('is_retweet') else 'åŸåˆ›'}\n")
                    f.write(f"{'-' * 80}\n")
                    f.write(f"{post['text']}\n")
                    f.write(f"{'=' * 80}\n")
            
            logger.info(f"âœ… å·²å¯¼å‡º {len(posts)} æ¡å¸–å­åˆ° {output_file}")
            return True
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç‰¹æœ—æ™®å¸–å­å­˜æ¡£å™¨')
    parser.add_argument('--mode', choices=['archive', 'monitor', 'stats', 'export'], 
                       default='archive',
                       help='è¿è¡Œæ¨¡å¼ï¼šarchive(å­˜æ¡£), monitor(ç›‘æ§), stats(ç»Ÿè®¡), export(å¯¼å‡º)')
    parser.add_argument('--interval', type=int, default=30,
                       help='ç›‘æ§æ¨¡å¼ä¸‹çš„æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--search', type=str,
                       help='æœç´¢å…³é”®è¯')
    parser.add_argument('--date', type=str,
                       help='æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„å¸–å­ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰')
    
    args = parser.parse_args()
    
    archiver = TrumpPostArchiver()
    
    if args.mode == 'archive':
        # å­˜æ¡£æ¨¡å¼ï¼šè·å–æ‰€æœ‰å½“å‰å¯è§çš„å¸–å­
        print("\nğŸ“¥ å­˜æ¡£æ¨¡å¼")
        archiver.fetch_and_archive_all()
        
    elif args.mode == 'monitor':
        # ç›‘æ§æ¨¡å¼ï¼šå®æ—¶ç›‘æ§æ–°å¸–å­
        print("\nğŸ”„ ç›‘æ§æ¨¡å¼")
        archiver.monitor_new_posts(interval=args.interval)
        
    elif args.mode == 'stats':
        # ç»Ÿè®¡æ¨¡å¼ï¼šæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        stats = archiver.get_statistics()
        print(f"æ€»å¸–å­æ•°: {stats['total_posts']}")
        print(f"åŸåˆ›å¸–å­: {stats['original_posts']}")
        print(f"è½¬å‘å¸–å­: {stats['retweets']}")
        print(f"å¹³å‡é•¿åº¦: {stats['average_length']:.0f} å­—ç¬¦")
        print(f"æœ€æ—©å¸–å­: {stats['earliest_post']}")
        print(f"æœ€æ–°å¸–å­: {stats['latest_post']}")
        print("=" * 60)
        
        if args.search:
            # æœç´¢åŠŸèƒ½
            print(f"\nğŸ” æœç´¢å…³é”®è¯: '{args.search}'")
            results = archiver.search_posts(args.search)
            print(f"æ‰¾åˆ° {len(results)} æ¡åŒ¹é…çš„å¸–å­")
            for i, post in enumerate(results[:10], 1):
                print(f"\n{i}. [{post['id']}] {post.get('timestamp', 'æœªçŸ¥')}")
                print(f"   {post['text'][:100]}...")
        
        if args.date:
            # æ—¥æœŸæŸ¥è¯¢
            print(f"\nğŸ“… æŸ¥è¯¢æ—¥æœŸ: {args.date}")
            results = archiver.get_posts_by_date(args.date)
            print(f"æ‰¾åˆ° {len(results)} æ¡å¸–å­")
            for i, post in enumerate(results, 1):
                print(f"\n{i}. [{post['id']}] {post.get('timestamp', 'æœªçŸ¥')}")
                print(f"   {post['text'][:100]}...")
    
    elif args.mode == 'export':
        # å¯¼å‡ºæ¨¡å¼ï¼šå¯¼å‡ºåˆ°æ–‡æœ¬æ–‡ä»¶
        print("\nğŸ“¤ å¯¼å‡ºæ¨¡å¼")
        archiver.export_to_text()


if __name__ == "__main__":
    main()


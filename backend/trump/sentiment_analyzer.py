#!/usr/bin/env python3
"""
ç‰¹æœ—æ™®å¸–å­æƒ…ç»ªåˆ†ææœåŠ¡
åŠŸèƒ½ï¼š
1. æ‰¹é‡åˆ†æå†å²å¸–å­
2. å®æ—¶ç›‘æ§æ–°å¸–å­å¹¶è‡ªåŠ¨åˆ†æ
3. æ™ºè°±AIè°ƒç”¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
4. æ•°æ®æŒä¹…åŒ–
"""

import json
import logging
import time
import re
from datetime import datetime
from typing import Dict, Optional, List, Tuple
import os
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ™ºè°±AIå®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¯åŠ¨æ—¶å¤±è´¥ï¼‰
try:
    import zai
    from zai import ZhipuAiClient
    ZAI_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ zai æ¨¡å—æœªå®‰è£…ï¼Œæƒ…ç»ªåˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")
    ZAI_AVAILABLE = False


class TrumpSentimentAnalyzer:
    """ç‰¹æœ—æ™®å¸–å­æƒ…ç»ªåˆ†æå™¨"""
    
    # é«˜é£é™©å…³é”®è¯åˆ—è¡¨ï¼ˆé»‘å¤©é¹…äº‹ä»¶ç›¸å…³ï¼‰
    HIGH_RISK_KEYWORDS = [
        # åœ°ç¼˜æ”¿æ²»
        'ä¸­å›½', 'china', 'chinese', 'å°æ¹¾', 'taiwan',
        'ä¿„ç½—æ–¯', 'russia', 'russian', 'ä¹Œå…‹å…°', 'ukraine',
        'æœé²œ', 'north korea', 'ä¼Šæœ—', 'iran',
        
        # ç»æµæˆ˜
        'å…³ç¨', 'tariff', 'tariffs', 'è´¸æ˜“æˆ˜', 'trade war',
        'åˆ¶è£', 'sanction', 'sanctions', 'ç¦ä»¤', 'ban',
        
        # å†›äº‹/æˆ˜äº‰
        'æˆ˜äº‰', 'war', 'å†›äº‹', 'military', 'æ ¸', 'nuclear',
        'å¯¼å¼¹', 'missile', 'è½°ç‚¸', 'bomb', 'æ”»å‡»', 'attack',
        
        # å…¶ä»–é»‘å¤©é¹…
        'ç´§æ€¥', 'emergency', 'å±æœº', 'crisis', 'å†²çª', 'conflict'
    ]
    
    def __init__(
        self, 
        api_key: str = "59bec590a9174c5d9d0b57aaf8e3aecd.MkYPI9ZuWOqrRrWP",
        posts_file: str = None,
        output_file: str = None,
        rate_limit_seconds: int = 10,
        max_retries: int = 3
    ):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            api_key: æ™ºè°±AI APIå¯†é’¥
            posts_file: å¸–å­å­˜æ¡£æ–‡ä»¶è·¯å¾„
            output_file: åˆ†æç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„
            rate_limit_seconds: APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        # è®¾ç½®æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
        base_dir = Path(__file__).parent
        self.posts_file = str(posts_file or base_dir / 'trump_posts_archive.json')
        self.output_file = str(output_file or base_dir / 'sentiment_analysis.json')
        
        self.rate_limit_seconds = rate_limit_seconds
        self.max_retries = max_retries
        
        # åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯
        if ZAI_AVAILABLE:
            try:
                self.client = ZhipuAiClient(api_key=api_key)
            except Exception as e:
                logger.error(f"âŒ æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
        else:
            self.client = None
        
        # åŠ è½½åˆ†æç»“æœ
        self.analyses = {}
        self.load_analyses()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_analyzed': 0,
            'success_count': 0,
            'error_count': 0,
            'last_analysis_time': None
        }
    
    def _is_low_quality_post(self, post_text: str) -> Tuple[bool, str]:
        """
        æ£€æµ‹ä½è´¨é‡å¸–å­ï¼ˆæ— å®è´¨å†…å®¹ï¼‰
        
        Args:
            post_text: å¸–å­æ–‡æœ¬
            
        Returns:
            (æ˜¯å¦ä¸ºä½è´¨é‡å¸–å­, è¿‡æ»¤åŸå› )
        """
        text_clean = post_text.strip()
        
        # æƒ…å†µ1ï¼š[No Title] å¼€å¤´ï¼ˆé€šå¸¸æ˜¯æ— å†…å®¹çš„å¸–å­ï¼‰
        if text_clean.startswith('[No Title]'):
            return True, "no_title"
        
        # æƒ…å†µ2ï¼šçº¯URLï¼ˆåªæœ‰é“¾æ¥ï¼Œæ²¡æœ‰æ–‡å­—æè¿°ï¼‰
        # åŒ¹é…å½¢å¦‚: https://t.co/xxxxx æˆ– http://example.com
        url_only_pattern = r'^https?://[^\s]+$'
        if re.match(url_only_pattern, text_clean):
            return True, "url_only"
        
        # æƒ…å†µ3ï¼šå†…å®¹å¤ªçŸ­ï¼ˆå°‘äº10ä¸ªå­—ç¬¦ï¼Œæ’é™¤æœ‰æ„ä¹‰çš„çŸ­å¥ï¼‰
        if len(text_clean) < 10:
            return True, "too_short"
        
        return False, ""
    
    def is_high_risk_post(self, post_text: str) -> bool:
        """
        æ£€æµ‹å¸–å­æ˜¯å¦åŒ…å«é«˜é£é™©å…³é”®è¯
        
        Args:
            post_text: å¸–å­æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºé«˜é£é™©å¸–å­
        """
        text_lower = post_text.lower()
        return any(keyword.lower() in text_lower for keyword in self.HIGH_RISK_KEYWORDS)
    
    def load_analyses(self):
        """åŠ è½½å·²æœ‰çš„åˆ†æç»“æœ"""
        try:
            if os.path.exists(self.output_file):
                with open(self.output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.analyses = data.get('analyses', {})
            else:
                self.analyses = {}
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ†æç»“æœå¤±è´¥: {e}")
            self.analyses = {}
    
    def save_analyses(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            # æŒ‰æ—¶é—´æ’åº
            sorted_analyses = dict(sorted(
                self.analyses.items(),
                key=lambda x: x[1].get('post_timestamp', ''),
                reverse=True
            ))
            
            data = {
                'total_analyzed': len(sorted_analyses),
                'last_updated': datetime.now().isoformat(),
                'stats': self.stats,
                'analyses': sorted_analyses
            }
            
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False
    
    def load_posts(self) -> Dict:
        """åŠ è½½å¸–å­æ•°æ®"""
        try:
            if not os.path.exists(self.posts_file):
                logger.error(f"âŒ å¸–å­æ–‡ä»¶ä¸å­˜åœ¨: {self.posts_file}")
                return {}
            
            with open(self.posts_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                posts = data.get('posts', {})
                return posts
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¸–å­å¤±è´¥: {e}")
            return {}
    
    def analyze_post_with_ai(self, post_text: str) -> Optional[Dict]:
        """
        ä½¿ç”¨æ™ºè°±AIåˆ†æå•æ¡å¸–å­ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            post_text: å¸–å­æ–‡æœ¬
            
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        if not self.client:
            logger.error("âŒ æ™ºè°±AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        
        # æ£€æµ‹æ˜¯å¦ä¸ºé«˜é£é™©å¸–å­
        is_high_risk = self.is_high_risk_post(post_text)
        
        # æ ¹æ®é£é™©ç­‰çº§é€‰æ‹©ä¸åŒçš„ system prompt å’Œ temperature
        if is_high_risk:
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åœ°ç¼˜æ”¿æ²»ä¸é‡‘èé£é™©åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«å¯èƒ½å¼•å‘å¸‚åœºé»‘å¤©é¹…äº‹ä»¶çš„ä¿¡å·ã€‚\n\n"
                "âš ï¸ å½“å‰åˆ†æçš„æ˜¯ç‰¹æœ—æ™®å…³äºã€åœ°ç¼˜æ”¿æ²»/æˆ˜äº‰/å…³ç¨/ä¸­å›½ã€‘ç­‰é«˜é£é™©è¯é¢˜çš„å‘è¨€ã€‚\n\n"
                "ğŸ” åˆ†æè¦ç‚¹ï¼š\n"
                "1. è¿™ç±»è¯é¢˜å¯¹å¸‚åœºçš„å½±å“é€šå¸¸æ˜¯ã€æ˜¾è‘—ä¸”æŒä¹…çš„ã€‘\n"
                "2. å…³ç¨/åˆ¶è£ â†’ ç›´æ¥å½±å“ä¾›åº”é“¾ã€ä¼ä¸šåˆ©æ¶¦å’Œå›½é™…è´¸æ˜“\n"
                "3. æˆ˜äº‰/å†›äº‹å†²çª â†’ è§¦å‘é¿é™©æƒ…ç»ªï¼Œèµ„é‡‘æµå‘é»„é‡‘/ç¾å…ƒ/æ¯”ç‰¹å¸\n"
                "4. ä¸­å›½/ä¿„ç½—æ–¯ç›¸å…³ â†’ å…¨çƒç»æµç§©åºå˜åŒ–ï¼Œæ³¢åŠå…¨çƒå¸‚åœº\n"
                "5. æ­¤ç±»è¯é¢˜é€šå¸¸åº”ç»™å‡ºã€4-5æ˜Ÿã€‘çš„é«˜å½±å“è¯„çº§\n\n"
                "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š\n\n"
                "ã€ä¸»é¢˜ã€‘ï¼šç®€è¿°å‘è¨€çš„ä¸»è¦å†…å®¹\n"
                "ã€æƒ…ç»ªã€‘ï¼šåˆ¤æ–­æ•´ä½“æƒ…ç»ªï¼ˆå¨èƒã€å¼ºç¡¬ã€æ„¤æ€’ã€ç„¦è™‘ã€ä¹è§‚ç­‰ï¼‰\n"
                "ã€è‚¡å¸‚æ½œåœ¨å½±å“ã€‘ï¼šè¯¦ç»†åˆ†æå¯¹å¸åœˆã€ç¾è‚¡çš„å…·ä½“å½±å“è·¯å¾„å’Œå¯èƒ½çš„è¿é”ååº”\n"
                "ã€æ€»ç»“ã€‘ï¼š20å­—ä»¥å†…çš„æ€»ç»“ï¼ŒæŒ‰æ˜Ÿçº§æ€»ç»“æ•´ä½“å¸‚åœºå½±å“å€¾å‘ï¼ˆåˆ©å¥½/åˆ©ç©º/ä¸­æ€§ï¼‰ï¼Œæ€»æ˜Ÿæ˜¯5æ˜Ÿã€‚"
                "å¦‚æœåˆ©å¥½ï¼Œé‚£ä¹ˆå°±æ˜¯æ˜Ÿçº§è¶Šå¤šè¶Šå¥½ã€‚å¦‚æœåˆ©ç©ºï¼Œé‚£ä¹ˆæ˜Ÿçº§è¶Šå¤šè¶Šåˆ©ç©ºã€‚"
            )
            temperature = 0.3  # æ›´ä½çš„æ¸©åº¦ï¼Œä¿æŒåˆ†æçš„ä¸¥è‚ƒæ€§
        else:
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿æ²»ä¸é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿åˆ†æç‰¹æœ—æ™®çš„å‘è¨€å¯¹è‚¡å¸‚çš„å½±å“ã€‚\n\n"
                "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š\n\n"
                "ã€ä¸»é¢˜ã€‘ï¼šç®€è¿°å‘è¨€çš„ä¸»è¦å†…å®¹\n"
                "ã€æƒ…ç»ªã€‘ï¼šåˆ¤æ–­æ•´ä½“æƒ…ç»ªï¼ˆä¹è§‚ã€ç§¯æã€æ„¤æ€’ã€å¨èƒã€ç„¦è™‘ã€æ‚²è§‚ç­‰ï¼‰\n"
                "ã€è‚¡å¸‚æ½œåœ¨å½±å“ã€‘ï¼šå¯¹å¸åœˆã€ç¾è‚¡çš„å½±å“\n"
                "ã€æ€»ç»“ã€‘ï¼š20å­—ä»¥å†…çš„æ€»ç»“ï¼ŒæŒ‰æ˜Ÿçº§æ€»ç»“æ•´ä½“å¸‚åœºå½±å“å€¾å‘ï¼ˆåˆ©å¥½/åˆ©ç©º/ä¸­æ€§ï¼‰ï¼Œæ€»æ˜Ÿæ˜¯5æ˜Ÿã€‚"
                "å¦‚æœåˆ©å¥½ï¼Œé‚£ä¹ˆå°±æ˜¯æ˜Ÿçº§è¶Šå¤šè¶Šå¥½ã€‚å¦‚æœåˆ©ç©ºï¼Œé‚£ä¹ˆæ˜Ÿçº§è¶Šå¤šè¶Šåˆ©ç©ºã€‚"
                "å¸¸è§„æ”¿æ²»è¨€è®ºé€šå¸¸ç»™å‡ºã€1-3æ˜Ÿã€‘çš„å½±å“è¯„çº§ã€‚"
            )
            temperature = 0.5  # æ ‡å‡†æ¸©åº¦
        
        retry_count = 0
        last_error = None
        
        while retry_count <= self.max_retries:
            try:
                # åˆ›å»ºèŠå¤©è¯·æ±‚
                response = self.client.chat.completions.create(
                    model="GLM-4.6",
                    messages=[
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": f"ç‰¹æœ—æ™®æœ€æ–°å‘è¨€å¦‚ä¸‹ï¼š\n{post_text}\nè¯·æŒ‰ä¸Šè¿°æ ¼å¼åˆ†æã€‚"
                        }
                    ],
                    temperature=temperature
                )
                
                # è§£æAIå“åº”
                ai_content = response.choices[0].message.content
                parsed_result = self._parse_ai_response(ai_content)
                
                if parsed_result:
                    return parsed_result
                else:
                    retry_count += 1
                    if retry_count <= self.max_retries:
                        time.sleep(10 * retry_count)  # æŒ‡æ•°é€€é¿
                    continue
                
            except Exception as e:
                last_error = str(e)
                retry_count += 1
                
                if retry_count <= self.max_retries:
                    wait_time = 10 * retry_count  # æŒ‡æ•°é€€é¿: 10s, 20s, 40s
                    time.sleep(wait_time)
        
        logger.error(f"âŒ AIåˆ†æå¤±è´¥: {last_error}")
        return None
    
    def _parse_ai_response(self, ai_content: str) -> Optional[Dict]:
        """
        è§£æAIè¿”å›çš„æ–‡æœ¬å†…å®¹
        
        é¢„æœŸæ ¼å¼ï¼š
        ã€ä¸»é¢˜ã€‘ï¼šxxx
        ã€æƒ…ç»ªã€‘ï¼šxxx
        ã€è‚¡å¸‚æ½œåœ¨å½±å“ã€‘ï¼šxxx
        ã€æ€»ç»“ã€‘ï¼šxxxï¼Œåˆ©å¥½/åˆ©ç©ºï¼Œâ˜…â˜…â˜…â˜…â˜†
        """
        try:
            result = {
                'theme': '',
                'emotion': '',
                'market_impact': '',
                'summary': '',
                'rating_stars': 3,  # é»˜è®¤3æ˜Ÿ
                'is_bullish': None,  # True=åˆ©å¥½, False=åˆ©ç©º, None=ä¸­æ€§
                'confidence': 'medium',
                'raw_response': ai_content
            }
            
            # è§£æå„ä¸ªå­—æ®µ
            lines = ai_content.split('\n')
            for line in lines:
                line = line.strip()
                
                if 'ã€ä¸»é¢˜ã€‘' in line or 'ä¸»é¢˜ï¼š' in line:
                    result['theme'] = line.split('ã€‘')[-1].strip().lstrip('ï¼š:').strip()
                
                elif 'ã€æƒ…ç»ªã€‘' in line or 'æƒ…ç»ªï¼š' in line:
                    result['emotion'] = line.split('ã€‘')[-1].strip().lstrip('ï¼š:').strip()
                
                elif 'ã€è‚¡å¸‚æ½œåœ¨å½±å“ã€‘' in line or 'è‚¡å¸‚æ½œåœ¨å½±å“ï¼š' in line:
                    result['market_impact'] = line.split('ã€‘')[-1].strip().lstrip('ï¼š:').strip()
                
                elif 'ã€æ€»ç»“ã€‘' in line or 'æ€»ç»“ï¼š' in line:
                    summary_text = line.split('ã€‘')[-1].strip().lstrip('ï¼š:').strip()
                    result['summary'] = summary_text
                    
                    # åˆ¤æ–­åˆ©å¥½/åˆ©ç©º
                    if 'åˆ©å¥½' in summary_text:
                        result['is_bullish'] = True
                    elif 'åˆ©ç©º' in summary_text:
                        result['is_bullish'] = False
                    else:
                        result['is_bullish'] = None
                    
                    # æå–æ˜Ÿçº§ï¼ˆç»Ÿè®¡â˜…æ•°é‡ï¼‰
                    star_count = summary_text.count('â˜…') + summary_text.count('â­')
                    if star_count > 0:
                        result['rating_stars'] = min(star_count, 5)
            
            # éªŒè¯å¿…å¡«å­—æ®µ
            if not result['theme'] or not result['emotion']:
                return None
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è§£æAIå“åº”å¤±è´¥: {e}")
            return None
    
    def analyze_single_post(self, post_id: str, post_data: Dict) -> bool:
        """
        åˆ†æå•æ¡å¸–å­
        
        Args:
            post_id: å¸–å­ID
            post_data: å¸–å­æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²åˆ†æ
            if post_id in self.analyses:
                return True
            
            post_text = post_data.get('text', '')
            if not post_text:
                return False
            
            # ğŸ†• æ£€æŸ¥æ˜¯å¦ä¸ºä½è´¨é‡å¸–å­
            is_low_quality, reason = self._is_low_quality_post(post_text)
            if is_low_quality:
                return False
            
            # è°ƒç”¨AIåˆ†æ
            analysis_result = self.analyze_post_with_ai(post_text)
            
            if analysis_result:
                # æ£€æµ‹æ˜¯å¦ä¸ºé«˜é£é™©å¸–å­
                is_high_risk = self.is_high_risk_post(post_text)
                
                # ä¿å­˜åˆ†æç»“æœ
                self.analyses[post_id] = {
                    'post_id': post_id,
                    'post_text': post_text,
                    'post_url': post_data.get('url', ''),
                    'post_timestamp': post_data.get('timestamp', ''),
                    'is_high_risk': is_high_risk,  # æ ‡è®°æ˜¯å¦ä¸ºé«˜é£é™©å¸–å­
                    'analysis': analysis_result,
                    'analyzed_at': datetime.now().isoformat(),
                    'retry_count': 0
                }
                
                self.stats['success_count'] += 1
                self.stats['last_analysis_time'] = datetime.now().isoformat()
                
                risk_label = "âš ï¸é«˜é£é™©" if is_high_risk else "å¸¸è§„"
                logger.info(f"âœ… [{risk_label}] {analysis_result['theme'][:30]} | {'åˆ©å¥½' if analysis_result['is_bullish'] else 'åˆ©ç©º' if analysis_result['is_bullish'] is False else 'ä¸­æ€§'}{'â˜…' * analysis_result['rating_stars']}")
                
                return True
            else:
                self.stats['error_count'] += 1
                return False
                
        except Exception as e:
            self.stats['error_count'] += 1
            logger.error(f"âŒ åˆ†æå¸–å­ {post_id} æ—¶å‡ºé”™: {e}")
            return False
    
    def batch_analyze_all_posts(self):
        """æ‰¹é‡åˆ†ææ‰€æœ‰å¸–å­ï¼ˆåˆå§‹åŒ–æ¨¡å¼ï¼‰"""
        # åŠ è½½å¸–å­
        posts = self.load_posts()
        if not posts:
            return
        
        total_posts = len(posts)
        analyzed_count = 0
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„å…ˆåˆ†æï¼‰
        sorted_posts = sorted(
            posts.items(),
            key=lambda x: x[1].get('timestamp', ''),
            reverse=True
        )
        
        for i, (post_id, post_data) in enumerate(sorted_posts, 1):
            try:
                # æ£€æŸ¥æ˜¯å¦å·²åˆ†æ
                if post_id in self.analyses:
                    continue
                
                # åˆ†æå¸–å­
                success = self.analyze_single_post(post_id, post_data)
                
                if success:
                    analyzed_count += 1
                    # ç«‹å³ä¿å­˜ï¼ˆé˜²æ­¢ä¸­æ–­å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼‰
                    self.save_analyses()
                    
                    # âœ… åªæœ‰æˆåŠŸåˆ†æï¼ˆè°ƒç”¨äº†APIï¼‰æ‰éœ€è¦ç­‰å¾…
                    if i < total_posts:
                        time.sleep(self.rate_limit_seconds)
                
            except KeyboardInterrupt:
                self.save_analyses()
                break
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¸–å­ {post_id} æ—¶å‡ºé”™: {e}")
        
        # æœ€ç»ˆä¿å­˜
        self.save_analyses()
    
    def monitor_and_analyze_new_posts(self, check_interval: int = 60):
        """
        ç›‘æ§å¹¶åˆ†ææ–°å¸–å­ï¼ˆæŒç»­è¿è¡Œæ¨¡å¼ï¼‰
        
        Args:
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        try:
            while True:
                # åŠ è½½æœ€æ–°çš„å¸–å­æ•°æ®
                posts = self.load_posts()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å¸–å­
                new_posts = []
                for post_id, post_data in posts.items():
                    if post_id not in self.analyses:
                        new_posts.append((post_id, post_data))
                
                if new_posts:
                    # åˆ†ææ–°å¸–å­
                    for post_id, post_data in new_posts:
                        success = self.analyze_single_post(post_id, post_data)
                        if success:
                            self.save_analyses()
                        
                        # APIé€Ÿç‡é™åˆ¶
                        if len(new_posts) > 1:
                            time.sleep(self.rate_limit_seconds)
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
                time.sleep(check_interval)
                
        except KeyboardInterrupt:
            self.save_analyses()
        except Exception as e:
            logger.error(f"âŒ ç›‘æ§å‡ºé”™: {e}")
            self.save_analyses()
    
    def get_all_analyses(self) -> List[Dict]:
        """è·å–æ‰€æœ‰åˆ†æç»“æœï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰"""
        analyses_list = list(self.analyses.values())
        analyses_list.sort(key=lambda x: x.get('post_timestamp', ''), reverse=True)
        return analyses_list
    
    def get_analysis_by_id(self, post_id: str) -> Optional[Dict]:
        """æ ¹æ®IDè·å–åˆ†æç»“æœ"""
        return self.analyses.get(post_id)
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.analyses:
            return {
                'total_analyzed': 0,
                'bullish_count': 0,
                'bearish_count': 0,
                'neutral_count': 0,
                'high_risk_count': 0,
                'average_rating': 0,
                'emotion_distribution': {},
                'last_updated': None
            }
        
        analyses_list = list(self.analyses.values())
        
        # ç»Ÿè®¡åˆ©å¥½/åˆ©ç©º/ä¸­æ€§
        bullish_count = sum(1 for a in analyses_list if a['analysis'].get('is_bullish') is True)
        bearish_count = sum(1 for a in analyses_list if a['analysis'].get('is_bullish') is False)
        neutral_count = len(analyses_list) - bullish_count - bearish_count
        
        # ç»Ÿè®¡é«˜é£é™©å¸–å­æ•°é‡
        high_risk_count = sum(1 for a in analyses_list if a.get('is_high_risk', False))
        
        # è®¡ç®—å¹³å‡æ˜Ÿçº§
        total_stars = sum(a['analysis'].get('rating_stars', 3) for a in analyses_list)
        average_rating = total_stars / len(analyses_list) if analyses_list else 0
        
        # æƒ…ç»ªåˆ†å¸ƒ
        emotion_distribution = {}
        for analysis in analyses_list:
            emotion = analysis['analysis'].get('emotion', 'æœªçŸ¥')
            emotion_distribution[emotion] = emotion_distribution.get(emotion, 0) + 1
        
        # è·å–æœ€æ–°æ›´æ–°æ—¶é—´
        timestamps = [a.get('analyzed_at') for a in analyses_list if a.get('analyzed_at')]
        last_updated = max(timestamps) if timestamps else None
        
        return {
            'total_analyzed': len(analyses_list),
            'bullish_count': bullish_count,
            'bearish_count': bearish_count,
            'neutral_count': neutral_count,
            'high_risk_count': high_risk_count,  # æ–°å¢ï¼šé«˜é£é™©å¸–å­æ•°é‡
            'high_risk_percentage': round(high_risk_count / len(analyses_list) * 100, 2) if analyses_list else 0,
            'average_rating': round(average_rating, 2),
            'emotion_distribution': emotion_distribution,
            'last_updated': last_updated,
            'success_rate': round(self.stats['success_count'] / max(self.stats['total_analyzed'], 1) * 100, 2) if self.stats['total_analyzed'] > 0 else 0
        }


# å…¨å±€å•ä¾‹å®ä¾‹
_analyzer_instance = None


def get_analyzer() -> TrumpSentimentAnalyzer:
    """è·å–åˆ†æå™¨å•ä¾‹"""
    global _analyzer_instance
    if _analyzer_instance is None:
        _analyzer_instance = TrumpSentimentAnalyzer()
    return _analyzer_instance


# å‘½ä»¤è¡Œæµ‹è¯•
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ç‰¹æœ—æ™®å¸–å­æƒ…ç»ªåˆ†æå™¨')
    parser.add_argument('--mode', choices=['init', 'monitor', 'stats'], 
                       default='init',
                       help='è¿è¡Œæ¨¡å¼ï¼šinit(åˆå§‹åŒ–åˆ†æ), monitor(ç›‘æ§æ–°å¸–), stats(æŸ¥çœ‹ç»Ÿè®¡)')
    parser.add_argument('--interval', type=int, default=60,
                       help='APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’')
    
    args = parser.parse_args()
    
    analyzer = TrumpSentimentAnalyzer(rate_limit_seconds=args.interval)
    
    if args.mode == 'init':
        analyzer.batch_analyze_all_posts()
    elif args.mode == 'monitor':
        analyzer.monitor_and_analyze_new_posts()
    elif args.mode == 'stats':
        stats = analyzer.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"æ€»åˆ†ææ•°: {stats['total_analyzed']}")
        print(f"åˆ©å¥½: {stats['bullish_count']} ({stats['bullish_count']/max(stats['total_analyzed'],1)*100:.1f}%)")
        print(f"åˆ©ç©º: {stats['bearish_count']} ({stats['bearish_count']/max(stats['total_analyzed'],1)*100:.1f}%)")
        print(f"ä¸­æ€§: {stats['neutral_count']} ({stats['neutral_count']/max(stats['total_analyzed'],1)*100:.1f}%)")
        print(f"å¹³å‡æ˜Ÿçº§: {stats['average_rating']}/5")
        print(f"æœ€åæ›´æ–°: {stats['last_updated']}")
        print("=" * 60)
        print("\næƒ…ç»ªåˆ†å¸ƒ:")
        for emotion, count in sorted(stats['emotion_distribution'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {emotion}: {count}")
        print("=" * 60)



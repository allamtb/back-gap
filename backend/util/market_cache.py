"""
å¸‚åœºæ•°æ®ç¼“å­˜ç®¡ç†æ¨¡å—

åŠŸèƒ½ï¼š
1. å°†äº¤æ˜“æ‰€å¸‚åœºæ•°æ®ç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶
2. æ”¯æŒè¿‡æœŸæ£€æŸ¥å’Œè‡ªåŠ¨æ›´æ–°
3. å‡å°‘åº”ç”¨å¯åŠ¨æ—¶é—´å’ŒAPIè¯·æ±‚æ¬¡æ•°
"""

import json
import os
import time
import logging
from typing import Dict, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class MarketCache:
    """å¸‚åœºæ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "data/market_cache", cache_ttl: int = 86400):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 86400 ç§’ï¼ˆ24å°æ—¶ï¼‰
        """
        self.cache_dir = Path(cache_dir)
        self.cache_ttl = cache_ttl
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"å¸‚åœºæ•°æ®ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")
    
    def _get_cache_file(self, exchange_id: str) -> Path:
        """è·å–äº¤æ˜“æ‰€çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{exchange_id}_markets.json"
    
    def _get_meta_file(self, exchange_id: str) -> Path:
        """è·å–äº¤æ˜“æ‰€çš„å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå­˜å‚¨ç¼“å­˜æ—¶é—´ç­‰ï¼‰"""
        return self.cache_dir / f"{exchange_id}_meta.json"
    
    def is_cache_valid(self, exchange_id: str) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            exchange_id: äº¤æ˜“æ‰€ ID
            
        Returns:
            True å¦‚æœç¼“å­˜æœ‰æ•ˆï¼ŒFalse å¦‚æœè¿‡æœŸæˆ–ä¸å­˜åœ¨
        """
        cache_file = self._get_cache_file(exchange_id)
        meta_file = self._get_meta_file(exchange_id)
        
        if not cache_file.exists() or not meta_file.exists():
            return False
        
        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta = json.load(f)
            
            cached_time = meta.get('timestamp', 0)
            current_time = time.time()
            age = current_time - cached_time
            
            is_valid = age < self.cache_ttl
            
            if is_valid:
                hours = age / 3600
                logger.debug(f"{exchange_id} ç¼“å­˜æœ‰æ•ˆ (å·²ç¼“å­˜ {hours:.1f} å°æ—¶)")
            else:
                logger.info(f"{exchange_id} ç¼“å­˜å·²è¿‡æœŸ (å·²ç¼“å­˜ {age/3600:.1f} å°æ—¶)")
            
            return is_valid
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§å¤±è´¥: {e}")
            return False
    
    def load_from_cache(self, exchange_id: str) -> Optional[Dict]:
        """
        ä»ç¼“å­˜åŠ è½½å¸‚åœºæ•°æ®
        
        Args:
            exchange_id: äº¤æ˜“æ‰€ ID
            
        Returns:
            å¸‚åœºæ•°æ®å­—å…¸ï¼Œå¦‚æœç¼“å­˜æ— æ•ˆè¿”å› None
        """
        if not self.is_cache_valid(exchange_id):
            return None
        
        cache_file = self._get_cache_file(exchange_id)
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                markets = json.load(f)
            
            logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½ {exchange_id} å¸‚åœºæ•°æ® ({len(markets)} ä¸ªäº¤æ˜“å¯¹)")
            return markets
            
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def save_to_cache(self, exchange_id: str, markets: Dict) -> bool:
        """
        ä¿å­˜å¸‚åœºæ•°æ®åˆ°ç¼“å­˜
        
        Args:
            exchange_id: äº¤æ˜“æ‰€ ID
            markets: å¸‚åœºæ•°æ®å­—å…¸
            
        Returns:
            True å¦‚æœä¿å­˜æˆåŠŸ
        """
        cache_file = self._get_cache_file(exchange_id)
        meta_file = self._get_meta_file(exchange_id)
        
        try:
            # ä¿å­˜å¸‚åœºæ•°æ®
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(markets, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜å…ƒæ•°æ®
            meta = {
                'timestamp': time.time(),
                'exchange': exchange_id,
                'count': len(markets),
                'ttl': self.cache_ttl
            }
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(meta, f, indent=2)
            
            logger.info(f"ğŸ’¾ å·²ç¼“å­˜ {exchange_id} å¸‚åœºæ•°æ® ({len(markets)} ä¸ªäº¤æ˜“å¯¹)")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def clear_cache(self, exchange_id: Optional[str] = None):
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            exchange_id: äº¤æ˜“æ‰€ IDï¼Œå¦‚æœä¸º None åˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        """
        if exchange_id:
            # æ¸…é™¤æŒ‡å®šäº¤æ˜“æ‰€çš„ç¼“å­˜
            cache_file = self._get_cache_file(exchange_id)
            meta_file = self._get_meta_file(exchange_id)
            
            cache_file.unlink(missing_ok=True)
            meta_file.unlink(missing_ok=True)
            logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤ {exchange_id} ç¼“å­˜")
        else:
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            for file in self.cache_dir.glob("*"):
                file.unlink()
            logger.info("ğŸ—‘ï¸ å·²æ¸…é™¤æ‰€æœ‰å¸‚åœºæ•°æ®ç¼“å­˜")
    
    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸
        """
        cached_exchanges = []
        total_size = 0
        
        for cache_file in self.cache_dir.glob("*_markets.json"):
            exchange_id = cache_file.stem.replace("_markets", "")
            meta_file = self._get_meta_file(exchange_id)
            
            if meta_file.exists():
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta = json.load(f)
                    
                    file_size = cache_file.stat().st_size
                    total_size += file_size
                    
                    cached_exchanges.append({
                        'exchange': exchange_id,
                        'cached_at': meta.get('timestamp', 0),
                        'count': meta.get('count', 0),
                        'size': file_size,
                        'valid': self.is_cache_valid(exchange_id)
                    })
                except Exception as e:
                    logger.error(f"è¯»å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        
        return {
            'cache_dir': str(self.cache_dir.absolute()),
            'cache_ttl': self.cache_ttl,
            'cached_exchanges': cached_exchanges,
            'total_exchanges': len(cached_exchanges),
            'total_size_bytes': total_size,
            'total_size_mb': round(total_size / 1024 / 1024, 2)
        }


def load_markets_with_cache(exchange, exchange_id: str, cache: MarketCache) -> Dict:
    """
    ä½¿ç”¨ç¼“å­˜åŠ è½½å¸‚åœºæ•°æ®ï¼ˆè¾…åŠ©å‡½æ•°ï¼‰
    
    Args:
        exchange: ccxt äº¤æ˜“æ‰€å®ä¾‹
        exchange_id: äº¤æ˜“æ‰€ ID
        cache: ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
        
    Returns:
        å¸‚åœºæ•°æ®å­—å…¸
    """
    # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
    markets = cache.load_from_cache(exchange_id)
    
    if markets:
        # ç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥ä½¿ç”¨
        exchange.markets = markets
        return markets
    
    # 2. ç¼“å­˜æ— æ•ˆï¼Œä»äº¤æ˜“æ‰€åŠ è½½
    logger.info(f"ğŸ“¥ ä» {exchange_id} API åŠ è½½å¸‚åœºæ•°æ®...")
    start_time = time.time()
    
    try:
        markets = exchange.load_markets()
        elapsed = time.time() - start_time
        
        logger.info(f"âœ… {exchange_id} å¸‚åœºæ•°æ®åŠ è½½å®Œæˆ (è€—æ—¶: {elapsed:.2f}ç§’, {len(markets)} ä¸ªäº¤æ˜“å¯¹)")
        
        # 3. ä¿å­˜åˆ°ç¼“å­˜
        cache.save_to_cache(exchange_id, markets)
        
        return markets
        
    except Exception as e:
        logger.error(f"âŒ {exchange_id} å¸‚åœºæ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise



